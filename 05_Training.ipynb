{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import google.datalab.bigquery as dlbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print (\"Built with cuda\")\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### From Jupyter notebook to python package\n",
    "From exploration to production.\n",
    "\n",
    "This little tool dumps a given function to a file with the same name in a certain package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE=\"./train\"\n",
    "from tools import make_src_dumper\n",
    "write_py = make_src_dumper(PACKAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Data\n",
    "Training and evaluation data should be provided in files already.\n",
    "\n",
    "If not, please go back an run ```Processing_ATL_JUNE.ipynb```\n",
    "\n",
    "#### Fetch a sample file for examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://going-tfx/atl_june/train_data/atl_june_csv-00000-of-00024...\n",
      "/ [1 files][143.0 KiB/143.0 KiB]                                                \n",
      "Operation completed over 1 objects/143.0 KiB.                                    \n",
      "\n",
      "1000 records in /tmp/atl_june/atl_june/atl_june_csv-00000-of-00024\n"
     ]
    }
   ],
   "source": [
    "DATASET='atl_june'\n",
    "a_training_file = !gsutil ls gs://going-tfx/$DATASET/train_data/atl_june_csv-00000-of-*\n",
    "a_training_file = a_training_file[0]\n",
    "TEMP_DIR='/tmp/atl_june/{}'.format(DATASET)\n",
    "!rm -rf $TEMP_DIR\n",
    "!mkdir -p $TEMP_DIR\n",
    "!gsutil cp $a_training_file $TEMP_DIR\n",
    "a_training_file = !ls $TEMP_DIR\n",
    "a_training_file = os.path.join(TEMP_DIR,a_training_file[0])\n",
    "res=!wc -l $a_training_file\n",
    "res=res[0].split(\" \")\n",
    "print()\n",
    "print(\"{} records in {}\".format(res[0], res[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a look into the first training data file\n",
    "\n",
    "This data is at the **training data** stage. It's got all and only the columns we want. Is has been normalized and integerized. We'll use the ```tf.feature_column``` API to further prepare categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>ARR</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>ARR_LAT</th>\n",
       "      <th>ARR_LON</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>DEP_DOW</th>\n",
       "      <th>DEP_HOD</th>\n",
       "      <th>DEP_LAT</th>\n",
       "      <th>DEP_LON</th>\n",
       "      <th>DIFF_LAT</th>\n",
       "      <th>DIFF_LON</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>MEAN_TEMP_ARR</th>\n",
       "      <th>MEAN_TEMP_DEP</th>\n",
       "      <th>MEAN_VIS_ARR</th>\n",
       "      <th>MEAN_VIS_DEP</th>\n",
       "      <th>WND_SPD_ARR</th>\n",
       "      <th>WND_SPD_DEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.36</td>\n",
       "      <td>-71.00</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.567288</td>\n",
       "      <td>0.933419</td>\n",
       "      <td>0.196172</td>\n",
       "      <td>0.315884</td>\n",
       "      <td>0.613445</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.007201</td>\n",
       "      <td>0.263566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>39.0</td>\n",
       "      <td>31.53</td>\n",
       "      <td>-84.19</td>\n",
       "      <td>0.127159</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.318150</td>\n",
       "      <td>0.791774</td>\n",
       "      <td>0.015087</td>\n",
       "      <td>0.604693</td>\n",
       "      <td>0.634454</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.542636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>26.07</td>\n",
       "      <td>-80.15</td>\n",
       "      <td>0.078493</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.192547</td>\n",
       "      <td>0.835159</td>\n",
       "      <td>0.113756</td>\n",
       "      <td>0.678700</td>\n",
       "      <td>0.634454</td>\n",
       "      <td>0.489247</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.534884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>25.79</td>\n",
       "      <td>-80.29</td>\n",
       "      <td>0.103611</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.186105</td>\n",
       "      <td>0.833655</td>\n",
       "      <td>0.116937</td>\n",
       "      <td>0.563177</td>\n",
       "      <td>0.613445</td>\n",
       "      <td>0.489247</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.263566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.03</td>\n",
       "      <td>-87.53</td>\n",
       "      <td>0.125589</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.467679</td>\n",
       "      <td>0.755906</td>\n",
       "      <td>0.061418</td>\n",
       "      <td>0.519856</td>\n",
       "      <td>0.613445</td>\n",
       "      <td>0.489247</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.294574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "      <td>30.0</td>\n",
       "      <td>33.67</td>\n",
       "      <td>-117.86</td>\n",
       "      <td>0.111460</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.367380</td>\n",
       "      <td>0.430198</td>\n",
       "      <td>0.415640</td>\n",
       "      <td>0.398917</td>\n",
       "      <td>0.634454</td>\n",
       "      <td>0.295699</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.317829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.81</td>\n",
       "      <td>-83.99</td>\n",
       "      <td>0.080063</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.416609</td>\n",
       "      <td>0.793922</td>\n",
       "      <td>0.016644</td>\n",
       "      <td>0.462094</td>\n",
       "      <td>0.634454</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006701</td>\n",
       "      <td>0.542636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>40.69</td>\n",
       "      <td>-74.16</td>\n",
       "      <td>0.073783</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.528870</td>\n",
       "      <td>0.899484</td>\n",
       "      <td>0.150884</td>\n",
       "      <td>0.326715</td>\n",
       "      <td>0.634454</td>\n",
       "      <td>0.198925</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011401</td>\n",
       "      <td>0.542636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>29.64</td>\n",
       "      <td>-95.27</td>\n",
       "      <td>0.073783</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.274672</td>\n",
       "      <td>0.672788</td>\n",
       "      <td>0.139405</td>\n",
       "      <td>0.671480</td>\n",
       "      <td>0.613445</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.007001</td>\n",
       "      <td>0.294574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>206.0</td>\n",
       "      <td>33.67</td>\n",
       "      <td>-78.92</td>\n",
       "      <td>0.395604</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.367380</td>\n",
       "      <td>0.848368</td>\n",
       "      <td>0.053715</td>\n",
       "      <td>0.548736</td>\n",
       "      <td>0.634454</td>\n",
       "      <td>0.397849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.542636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     AIRLINE  ARR  ARR_DELAY  ARR_LAT  ARR_LON  DEP_DELAY  DEP_DOW  DEP_HOD  \\\n",
       "97         0   12        1.0    42.36   -71.00   0.076923        5       13   \n",
       "693        1  106       39.0    31.53   -84.19   0.127159        2        8   \n",
       "777        0    6      -21.0    26.07   -80.15   0.078493        1        8   \n",
       "268        3    7        8.0    25.79   -80.29   0.103611        5       10   \n",
       "155        1   99       12.0    38.03   -87.53   0.125589        2       21   \n",
       "960        0  107       30.0    33.67  -117.86   0.111460        2       18   \n",
       "983        0   49        2.0    35.81   -83.99   0.080063        2       20   \n",
       "772        6    3      -15.0    40.69   -74.16   0.073783        2        9   \n",
       "182        5   23      -24.0    29.64   -95.27   0.073783        2        9   \n",
       "890        1   70      206.0    33.67   -78.92   0.395604        2       17   \n",
       "\n",
       "     DEP_LAT  DEP_LON  DIFF_LAT  DIFF_LON  DISTANCE  MEAN_TEMP_ARR  \\\n",
       "97     33.63   -84.42  0.567288  0.933419  0.196172       0.315884   \n",
       "693    33.63   -84.42  0.318150  0.791774  0.015087       0.604693   \n",
       "777    33.63   -84.42  0.192547  0.835159  0.113756       0.678700   \n",
       "268    33.63   -84.42  0.186105  0.833655  0.116937       0.563177   \n",
       "155    33.63   -84.42  0.467679  0.755906  0.061418       0.519856   \n",
       "960    33.63   -84.42  0.367380  0.430198  0.415640       0.398917   \n",
       "983    33.63   -84.42  0.416609  0.793922  0.016644       0.462094   \n",
       "772    33.63   -84.42  0.528870  0.899484  0.150884       0.326715   \n",
       "182    33.63   -84.42  0.274672  0.672788  0.139405       0.671480   \n",
       "890    33.63   -84.42  0.367380  0.848368  0.053715       0.548736   \n",
       "\n",
       "     MEAN_TEMP_DEP  MEAN_VIS_ARR  MEAN_VIS_DEP  WND_SPD_ARR  WND_SPD_DEP  \n",
       "97        0.613445      0.483871      0.671875     0.007201     0.263566  \n",
       "693       0.634454      0.403226      1.000000     0.008301     0.542636  \n",
       "777       0.634454      0.489247      0.984375     0.003800     0.534884  \n",
       "268       0.613445      0.489247      0.671875     0.004800     0.263566  \n",
       "155       0.613445      0.489247      0.953125     0.002700     0.294574  \n",
       "960       0.634454      0.295699      0.984375     0.004300     0.317829  \n",
       "983       0.634454      0.370968      1.000000     0.006701     0.542636  \n",
       "772       0.634454      0.198925      1.000000     0.011401     0.542636  \n",
       "182       0.613445      0.483871      0.953125     0.007001     0.294574  \n",
       "890       0.634454      0.397849      1.000000     0.009701     0.542636  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train.model_config import ORDERED_TRAINING_COLUMNS\n",
    "probe = pd.read_csv(a_training_file, names=ORDERED_TRAINING_COLUMNS)\n",
    "probe.sample(frac=1.0)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>ARR</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>ARR_LAT</th>\n",
       "      <th>ARR_LON</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>DEP_DOW</th>\n",
       "      <th>DEP_HOD</th>\n",
       "      <th>DEP_LAT</th>\n",
       "      <th>DEP_LON</th>\n",
       "      <th>DIFF_LAT</th>\n",
       "      <th>DIFF_LON</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>MEAN_TEMP_ARR</th>\n",
       "      <th>MEAN_TEMP_DEP</th>\n",
       "      <th>MEAN_VIS_ARR</th>\n",
       "      <th>MEAN_VIS_DEP</th>\n",
       "      <th>WND_SPD_ARR</th>\n",
       "      <th>WND_SPD_DEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.224000</td>\n",
       "      <td>43.887000</td>\n",
       "      <td>11.20800</td>\n",
       "      <td>35.64568</td>\n",
       "      <td>-86.491930</td>\n",
       "      <td>0.098691</td>\n",
       "      <td>3.571000</td>\n",
       "      <td>14.435000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.412829</td>\n",
       "      <td>0.767054</td>\n",
       "      <td>0.128623</td>\n",
       "      <td>0.528343</td>\n",
       "      <td>0.597059</td>\n",
       "      <td>0.435871</td>\n",
       "      <td>0.850750</td>\n",
       "      <td>0.007501</td>\n",
       "      <td>0.390248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.176364</td>\n",
       "      <td>36.665059</td>\n",
       "      <td>33.44885</td>\n",
       "      <td>5.36921</td>\n",
       "      <td>12.619807</td>\n",
       "      <td>0.046621</td>\n",
       "      <td>1.703485</td>\n",
       "      <td>4.633276</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.123515</td>\n",
       "      <td>0.135522</td>\n",
       "      <td>0.115086</td>\n",
       "      <td>0.126634</td>\n",
       "      <td>0.105001</td>\n",
       "      <td>0.074737</td>\n",
       "      <td>0.160310</td>\n",
       "      <td>0.031538</td>\n",
       "      <td>0.134370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-30.00000</td>\n",
       "      <td>18.33000</td>\n",
       "      <td>-157.920000</td>\n",
       "      <td>0.047096</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063177</td>\n",
       "      <td>0.445378</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.062015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>-7.00000</td>\n",
       "      <td>30.69000</td>\n",
       "      <td>-89.970000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.298827</td>\n",
       "      <td>0.729704</td>\n",
       "      <td>0.062598</td>\n",
       "      <td>0.450812</td>\n",
       "      <td>0.466386</td>\n",
       "      <td>0.408602</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.294574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>35.87000</td>\n",
       "      <td>-82.530000</td>\n",
       "      <td>0.080063</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.417989</td>\n",
       "      <td>0.809601</td>\n",
       "      <td>0.105027</td>\n",
       "      <td>0.554152</td>\n",
       "      <td>0.613445</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.006101</td>\n",
       "      <td>0.426357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>18.00000</td>\n",
       "      <td>40.49000</td>\n",
       "      <td>-78.920000</td>\n",
       "      <td>0.102433</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.524270</td>\n",
       "      <td>0.848368</td>\n",
       "      <td>0.150528</td>\n",
       "      <td>0.615523</td>\n",
       "      <td>0.634454</td>\n",
       "      <td>0.489247</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.008201</td>\n",
       "      <td>0.519380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>292.00000</td>\n",
       "      <td>47.45000</td>\n",
       "      <td>-64.970000</td>\n",
       "      <td>0.500785</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.684380</td>\n",
       "      <td>0.998174</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.489247</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.542636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           AIRLINE          ARR   ARR_DELAY     ARR_LAT      ARR_LON  \\\n",
       "count  1000.000000  1000.000000  1000.00000  1000.00000  1000.000000   \n",
       "mean      1.224000    43.887000    11.20800    35.64568   -86.491930   \n",
       "std       2.176364    36.665059    33.44885     5.36921    12.619807   \n",
       "min       0.000000     0.000000   -30.00000    18.33000  -157.920000   \n",
       "25%       0.000000    13.000000    -7.00000    30.69000   -89.970000   \n",
       "50%       1.000000    36.000000     1.00000    35.87000   -82.530000   \n",
       "75%       2.000000    68.000000    18.00000    40.49000   -78.920000   \n",
       "max      16.000000   169.000000   292.00000    47.45000   -64.970000   \n",
       "\n",
       "         DEP_DELAY      DEP_DOW      DEP_HOD  DEP_LAT  DEP_LON     DIFF_LAT  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.00  1000.00  1000.000000   \n",
       "mean      0.098691     3.571000    14.435000    33.63   -84.42     0.412829   \n",
       "std       0.046621     1.703485     4.633276     0.00     0.00     0.123515   \n",
       "min       0.047096     1.000000     5.000000    33.63   -84.42     0.014493   \n",
       "25%       0.076923     2.000000    10.000000    33.63   -84.42     0.298827   \n",
       "50%       0.080063     4.000000    14.000000    33.63   -84.42     0.417989   \n",
       "75%       0.102433     4.000000    19.000000    33.63   -84.42     0.524270   \n",
       "max       0.500785     7.000000    23.000000    33.63   -84.42     0.684380   \n",
       "\n",
       "          DIFF_LON     DISTANCE  MEAN_TEMP_ARR  MEAN_TEMP_DEP  MEAN_VIS_ARR  \\\n",
       "count  1000.000000  1000.000000    1000.000000    1000.000000   1000.000000   \n",
       "mean      0.767054     0.128623       0.528343       0.597059      0.435871   \n",
       "std       0.135522     0.115086       0.126634       0.105001      0.074737   \n",
       "min       0.000000     0.000000       0.063177       0.445378      0.010753   \n",
       "25%       0.729704     0.062598       0.450812       0.466386      0.408602   \n",
       "50%       0.809601     0.105027       0.554152       0.613445      0.473118   \n",
       "75%       0.848368     0.150528       0.615523       0.634454      0.489247   \n",
       "max       0.998174     1.000000       1.000000       0.970588      0.489247   \n",
       "\n",
       "       MEAN_VIS_DEP  WND_SPD_ARR  WND_SPD_DEP  \n",
       "count   1000.000000  1000.000000  1000.000000  \n",
       "mean       0.850750     0.007501     0.390248  \n",
       "std        0.160310     0.031538     0.134370  \n",
       "min        0.484375     0.001000     0.062015  \n",
       "25%        0.687500     0.004500     0.294574  \n",
       "50%        0.953125     0.006101     0.426357  \n",
       "75%        0.984375     0.008201     0.519380  \n",
       "max        1.000000     1.000000     0.542636  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature engineering for categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical columns need to be treated once more to derive at numerical input suitable for model training. That involves bucketizing, the use of dictionaries, feature crossing and embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find ranges to bucketize latitude and longitude \n",
    "We can easily understand the range of values with the help of a bq query and ```pandas.describe()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>344.000000</td>\n",
       "      <td>344.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.491570</td>\n",
       "      <td>-98.531599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.547964</td>\n",
       "      <td>21.746974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.480000</td>\n",
       "      <td>-176.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.450000</td>\n",
       "      <td>-111.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38.715000</td>\n",
       "      <td>-93.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42.907500</td>\n",
       "      <td>-82.497500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>71.280000</td>\n",
       "      <td>-64.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              lat         lon\n",
       "count  344.000000  344.000000\n",
       "mean    38.491570  -98.531599\n",
       "std      8.547964   21.746974\n",
       "min     13.480000 -176.640000\n",
       "25%     33.450000 -111.675000\n",
       "50%     38.715000  -93.300000\n",
       "75%     42.907500  -82.497500\n",
       "max     71.280000  -64.800000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"\"\"\n",
    "select \n",
    "    distinct arrival_airport as airport, arrival_lat as lat, arrival_lon as lon \n",
    "from \n",
    "    `bigquery-samples.airline_ontime_data.flights`\n",
    "\"\"\"\n",
    "locations = dlbq.Query(query).execute().result().to_dataframe()\n",
    "locations.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_boundaries = range(10,80,5)\n",
    "lat_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100, -95, -90, -85, -80, -75, -70, -65, -60]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lon_boundaries = range(-100, -55, 5)\n",
    "lon_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use those boundaries in the function below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using tf feature_column api for bucketizing, crossing and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'create_feature_columns written to ./train/create_feature_columns.py.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_feature_columns():\n",
    "    \"\"\"\n",
    "        returns: a dict of features columns for wide and deep input\n",
    "    \"\"\"\n",
    "    \n",
    "    from tensorflow.feature_column import indicator_column as ind\n",
    "    from tensorflow.feature_column import numeric_column as num\n",
    "    from tensorflow.feature_column import bucketized_column as buck\n",
    "    from tensorflow.feature_column import crossed_column as cross\n",
    "    from tensorflow.feature_column import embedding_column as emb\n",
    "    from tensorflow.feature_column import categorical_column_with_identity as cid\n",
    "    \n",
    "    ################################################################\n",
    "    #  Numerical columns for the pre-processed features\n",
    "    ################################################################\n",
    "    feature_columns = [\n",
    "        num(col) for col in [\n",
    "            'DEP_DELAY',  \n",
    "            'MEAN_TEMP_DEP','MEAN_VIS_DEP','WND_SPD_DEP',\n",
    "            'MEAN_TEMP_ARR','MEAN_VIS_ARR','WND_SPD_ARR',\n",
    "            'DIFF_LAT','DIFF_LON','DISTANCE']]\n",
    "    \n",
    "    ################################################################\n",
    "    #  categorical from ints, bucket counts from examination of the \n",
    "    #  full dataset\n",
    "    ################################################################\n",
    "    airline = ind(cid('AIRLINE', num_buckets=30))\n",
    "    arrival = ind(cid('ARR', num_buckets=400))\n",
    "    \n",
    "    ################################################################\n",
    "    #  Crossed and embedded\n",
    "    ################################################################\n",
    "    lat_boundaries = range(10,80,5)\n",
    "    lon_boundaries = range(-100, -55, 5)\n",
    "    cross_size = len(lat_boundaries) * len(lon_boundaries)\n",
    "\n",
    "    arr_geo_emb = emb(cross([\n",
    "        buck(num('ARR_LAT'), lat_boundaries), \n",
    "        buck(num('ARR_LON'), lon_boundaries)], cross_size), 10)\n",
    "\n",
    "    dep_geo_emb = emb(cross([\n",
    "        buck(num(\"DEP_LAT\"), lat_boundaries), \n",
    "        buck(num(\"DEP_LON\"), lon_boundaries)], cross_size), 10)\n",
    "\n",
    "    dep_how_emb = emb(cross([\n",
    "        cid(\"DEP_HOD\", num_buckets=24), \n",
    "        cid(\"DEP_DOW\", num_buckets=8)], 7*24), 10)\n",
    "\n",
    "    ################################################################\n",
    "    #  all together\n",
    "    ################################################################\n",
    "    return {\n",
    "        'deep': feature_columns + [dep_how_emb, arr_geo_emb, dep_geo_emb],\n",
    "        'wide': [airline, arrival]}\n",
    "    \n",
    "write_py(create_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deep': [_NumericColumn(key='DEP_DELAY', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       "  _NumericColumn(key='MEAN_TEMP_DEP', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       "  _NumericColumn(key='MEAN_VIS_DEP', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       "  _NumericColumn(key='WND_SPD_DEP', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       "  _NumericColumn(key='MEAN_TEMP_ARR', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       "  _NumericColumn(key='MEAN_VIS_ARR', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       "  _NumericColumn(key='WND_SPD_ARR', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       "  _NumericColumn(key='DIFF_LAT', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       "  _NumericColumn(key='DIFF_LON', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       "  _NumericColumn(key='DISTANCE', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       "  _EmbeddingColumn(categorical_column=_CrossedColumn(keys=(_IdentityCategoricalColumn(key='DEP_HOD', num_buckets=24, default_value=None), _IdentityCategoricalColumn(key='DEP_DOW', num_buckets=8, default_value=None)), hash_bucket_size=168, hash_key=None), dimension=10, combiner='mean', layer_creator=<function _creator at 0x7f77b46552a8>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True),\n",
       "  _EmbeddingColumn(categorical_column=_CrossedColumn(keys=(_BucketizedColumn(source_column=_NumericColumn(key='ARR_LAT', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75)), _BucketizedColumn(source_column=_NumericColumn(key='ARR_LON', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(-100, -95, -90, -85, -80, -75, -70, -65, -60))), hash_bucket_size=126, hash_key=None), dimension=10, combiner='mean', layer_creator=<function _creator at 0x7f77d3f505f0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True),\n",
       "  _EmbeddingColumn(categorical_column=_CrossedColumn(keys=(_BucketizedColumn(source_column=_NumericColumn(key='DEP_LAT', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75)), _BucketizedColumn(source_column=_NumericColumn(key='DEP_LON', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(-100, -95, -90, -85, -80, -75, -70, -65, -60))), hash_bucket_size=126, hash_key=None), dimension=10, combiner='mean', layer_creator=<function _creator at 0x7f77b47c0b90>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)],\n",
       " 'wide': [_IndicatorColumn(categorical_column=_IdentityCategoricalColumn(key='AIRLINE', num_buckets=30, default_value=None)),\n",
       "  _IndicatorColumn(categorical_column=_IdentityCategoricalColumn(key='ARR', num_buckets=400, default_value=None))]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_feature_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These feature columns encode a construction plan. The ```tf.feature_column.input_column()``` helper will construct a sub-graph from this plan and feed the root (the *result*) of the graph into the model. You see the pattern: All parts of the tensor graph are created within the session/graph context of the ```Estimator``` API. Never outside of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The available hypotheses\n",
    "Please see [04_Hypotheses.ipynb](04_Hypotheses.ipynb) for more insight into the various hypotheses functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def hypothesis_linear(features, feature_columns, options):\n",
      "    \n",
      "    import tensorflow as tf\n",
      "    from train.train_tools import weight_summary\n",
      "\n",
      "    with tf.name_scope('Linear'):\n",
      "    \n",
      "        all_feature_columns = feature_columns['wide'] + feature_columns['deep']\n",
      "\n",
      "        input_layer = tf.feature_column.input_layer( \n",
      "            features, feature_columns=all_feature_columns)\n",
      "\n",
      "        out = tf.layers.dense(input_layer, 1, activation=None)\n",
      "        weight_summary(out)\n",
      "    \n",
      "    return out\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from train.make_hypotheses import make_hypotheses\n",
    "all_hypotheses = make_hypotheses()\n",
    "import inspect\n",
    "print(inspect.getsource(all_hypotheses['linear']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model function\n",
    "The model function is responsible for providing different variants of the actual model suitable for training, evaluation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make_model_fn written to ./train/make_model_fn.py.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_model_fn(feature_columns, options, hypothesis):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from train.make_hypotheses import make_hypotheses\n",
    "    \n",
    "    optimizers={\n",
    "        \"sgd\": tf.train.GradientDescentOptimizer(learning_rate=options['learning_rate']),\n",
    "        \"adam\": tf.train.AdamOptimizer(learning_rate=options['learning_rate']),\n",
    "        \"adagrad\": tf.train.AdagradOptimizer(learning_rate=options['learning_rate'])\n",
    "    }\n",
    "    \n",
    "    def _model_fn(features, labels, mode):\n",
    "\n",
    "        out = hypothesis(features, feature_columns, options)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=out)\n",
    "\n",
    "\n",
    "        labels = tf.expand_dims(labels, -1)\n",
    "        loss = tf.losses.mean_squared_error(labels, out)\n",
    "        mean_error=tf.metrics.mean(tf.abs(labels-out))\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:    \n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss = loss,\n",
    "                eval_metric_ops={'mean_error': mean_error}\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            optimizer = optimizers[options['optimizer']]\n",
    "            train_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "            grads = optimizer.compute_gradients(loss)\n",
    "            for g in grads:\n",
    "                name = \"%s-grad\" % g[1].name\n",
    "                name = name.replace(\":\", \"_\")\n",
    "                tf.summary.histogram(name, g[0])\n",
    "            \n",
    "            return tf.estimator.EstimatorSpec(  \n",
    "                mode,\n",
    "                loss = loss,\n",
    "                train_op = train_op)\n",
    "        \n",
    "    return _model_fn\n",
    "\n",
    "write_py(make_model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Available input functions\n",
    "Please examine **```Input_Functions.ipynb```** for more information about the available input functions.\n",
    "\n",
    "For now, have a look at the tfrecord input function we're going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def make_tfr_input_fn(filename_pattern, batch_size, options):\n",
      "    \n",
      "    import tensorflow as tf\n",
      "    from train.model_config import LABEL_COLUMN\n",
      "    from train.model_config import TRAINING_METADATA\n",
      "\n",
      "    feature_spec = TRAINING_METADATA.schema.as_feature_spec()\n",
      "\n",
      "    def _input_fn():\n",
      "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
      "            file_pattern=filename_pattern,\n",
      "            batch_size=batch_size,\n",
      "            features=feature_spec,\n",
      "            shuffle_buffer_size=options['shuffle_buffer_size'],\n",
      "            prefetch_buffer_size=options['prefetch_buffer_size'],\n",
      "            reader_num_threads=options['reader_num_threads'],\n",
      "            parser_num_threads=options['parser_num_threads'],\n",
      "            sloppy_ordering=options['sloppy_ordering'],\n",
      "            label_key=LABEL_COLUMN)\n",
      "\n",
      "        if options['distribute']:\n",
      "            return dataset \n",
      "        else:\n",
      "            return dataset.make_one_shot_iterator().get_next()\n",
      "    return _input_fn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from train.make_input_fns import make_input_fns\n",
    "tfr_input_fn = make_input_fns()['tfr']\n",
    "import inspect\n",
    "print(inspect.getsource(tfr_input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Serving input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make_tft_serving_input_fn written to ./train/make_tft_serving_input_fn.py.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_tft_serving_input_fn(metadata_dir):\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_transform as tft\n",
    "    from train.model_config import SIGNATURE_INT_COLUMNS\n",
    "    from train.model_config import SIGNATURE_FLOAT_COLUMNS\n",
    "    from train.model_config import SIGNATURE_STR_COLUMNS\n",
    "        \n",
    "    def _input_fn():\n",
    "        # placeholders for all the raw inputs\n",
    "        placeholders = {\n",
    "            key: tf.placeholder(name = key, shape=[None], dtype=tf.int64)\n",
    "            for key in SIGNATURE_INT_COLUMNS\n",
    "        }\n",
    "        placeholders.update({\n",
    "            key: tf.placeholder(name = key, shape=[None], dtype=tf.float32)\n",
    "            for key in SIGNATURE_FLOAT_COLUMNS\n",
    "        })\n",
    "\n",
    "        placeholders.update({\n",
    "            key: tf.placeholder(name = key, shape=[None], dtype=tf.string)\n",
    "            for key in SIGNATURE_STR_COLUMNS\n",
    "        })\n",
    "\n",
    "        # transform using the saved model in transform_fn        \n",
    "        transform_output = tft.TFTransformOutput(transform_output_dir=metadata_dir)\n",
    "        features = transform_output.transform_raw_features(placeholders)\n",
    "            \n",
    "        return tf.estimator.export.ServingInputReceiver(features, placeholders)\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "write_py(make_tft_serving_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_and_evaluate written to ./train/train_and_evaluate.py.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_and_evaluate(options):\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.estimator import RunConfig\n",
    "    from tensorflow.contrib.distribute import MirroredStrategy\n",
    "    import mlflow\n",
    "    \n",
    "    from train.make_model_fn import make_model_fn\n",
    "    from train.make_tft_serving_input_fn import make_tft_serving_input_fn\n",
    "    from train.create_feature_columns import create_feature_columns\n",
    "    from train.make_tfr_input_fn import make_tfr_input_fn\n",
    "    from train.make_hypotheses import make_hypotheses\n",
    "    from train.make_input_fns import make_input_fns\n",
    "\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "\n",
    "        log_params = [\n",
    "            'base_dir',\n",
    "            'file_format',\n",
    "            'train_batch_size',\n",
    "            'max_train_steps',\n",
    "            'reader_num_threads',\n",
    "            'parser_num_threads',\n",
    "            'prefetch_buffer_size'    \n",
    "        ]\n",
    "        \n",
    "        for key in log_params:\n",
    "            mlflow.log_param(key, options[key])\n",
    "\n",
    "        ##################################################################\n",
    "        #   Train and Eval Input Functions\n",
    "        ##################################################################\n",
    "        make_input_fn=make_input_fns()[options['file_format']]\n",
    "\n",
    "        train_input_fn = make_input_fn(options['train_data_pattern'], \n",
    "                                       options['train_batch_size'],\n",
    "                                       options)    \n",
    "\n",
    "        eval_input_fn = make_input_fn(options['eval_data_pattern'], \n",
    "                                      options['eval_batch_size'],\n",
    "                                      options)\n",
    "\n",
    "\n",
    "        ##################################################################\n",
    "        #   Create the hypothesis and the model_fn\n",
    "        ##################################################################\n",
    "        hypothesis = make_hypotheses()[options['hypothesis']]    \n",
    "        feature_columns = create_feature_columns()\n",
    "        model_fn = make_model_fn(feature_columns, options, hypothesis )\n",
    "\n",
    "\n",
    "        ##################################################################\n",
    "        #    Train and Eval Spec\n",
    "        ##################################################################\n",
    "        serving_input_fn = make_tft_serving_input_fn(options['metadata_dir'])\n",
    "        exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "\n",
    "        train_spec = tf.estimator.TrainSpec(\n",
    "            input_fn=train_input_fn, \n",
    "            max_steps=options['max_train_steps'])\n",
    "\n",
    "        eval_spec = tf.estimator.EvalSpec(\n",
    "            input_fn=eval_input_fn, exporters=exporter,\n",
    "            steps = options['eval_steps'],\n",
    "            throttle_secs=options['throttle_secs'],\n",
    "            start_delay_secs=0)\n",
    "\n",
    "\n",
    "        ##################################################################\n",
    "        #   Create and configure the estimator\n",
    "        ##################################################################\n",
    "        strategy = MirroredStrategy() if options['distribute'] else None\n",
    "        config = RunConfig(model_dir=options['model_dir'],\n",
    "                           save_summary_steps=options['save_summary_steps'],\n",
    "                           train_distribute=strategy, \n",
    "                           save_checkpoints_steps=options['save_checkpoints_steps'],\n",
    "                           log_step_count_steps=options['log_step_count_steps'])\n",
    "\n",
    "        estimator = tf.estimator.Estimator(\n",
    "                config=config,\n",
    "                model_fn=model_fn)\n",
    "\n",
    "\n",
    "        ##################################################################\n",
    "        #   Finally, train and evaluate the model\n",
    "        ##################################################################\n",
    "        final_eval = tf.estimator.train_and_evaluate(\n",
    "            estimator, \n",
    "            train_spec=train_spec, \n",
    "            eval_spec=eval_spec)\n",
    "        \n",
    "        if final_eval[0] is not None:\n",
    "            mlflow.log_metric('loss', final_eval[0]['loss'])\n",
    "            mlflow.log_metric('mean_error', final_eval[0]['mean_error'])\n",
    "\n",
    "        return final_eval\n",
    "        \n",
    "    \n",
    "write_py(train_and_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run from within the notebook kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using directory gs://going-tfx/atl_june/model/v0 to store the model.\n",
      "Remove the directory if you want to start from scratch\n",
      "=====================================================================================================\n",
      "export PYTHONPATH=${PYTHONPATH}:${PWD}\n",
      "python -m train.task \\\n",
      "  --eval_steps=\"10\"  \\\n",
      "  --parser_num_threads=\"16\"  \\\n",
      "  --eval_data_pattern=\"gs://going-tfx/atl_june/eval_data/atl_june_tfr*\"  \\\n",
      "  --train_batch_size=\"500\"  \\\n",
      "  --shuffle_buffer_size=\"10000\"  \\\n",
      "  --eval_batch_size=\"1024\"  \\\n",
      "  --sloppy_ordering=\"True\"  \\\n",
      "  --reader_num_threads=\"16\"  \\\n",
      "  --file_format=\"tfr\"  \\\n",
      "  --log_step_count_steps=\"200\"  \\\n",
      "  --model_dir=\"gs://going-tfx/atl_june/model/v0\"  \\\n",
      "  --throttle_secs=\"30\"  \\\n",
      "  --optimizer=\"sgd\"  \\\n",
      "  --learning_rate=\"0.001\"  \\\n",
      "  --hypothesis=\"linear\"  \\\n",
      "  --save_summary_steps=\"100\"  \\\n",
      "  --max_train_steps=\"40000\"  \\\n",
      "  --prefetch_buffer_size=\"10000\"  \\\n",
      "  --metadata_dir=\"gs://going-tfx/atl_june/metadata\"  \\\n",
      "  --train_data_pattern=\"gs://going-tfx/atl_june/train_data/atl_june_tfr*\"  \\\n",
      "  --save_checkpoints_steps=\"5000\"  \\\n",
      "  --base_dir=\"gs://going-tfx/atl_june\"  \\\n",
      "=====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from train.train_tools import join_paths\n",
    "\n",
    "DATASET='atl_june'\n",
    "\n",
    "args={}\n",
    "\n",
    "# file locations\n",
    "args['base_dir']='gs://going-tfx/{}'.format(DATASET)\n",
    "args['metadata_dir']='metadata'\n",
    "args['model_dir']='model/v0'\n",
    "args['train_data_pattern']='train_data/atl_june_tfr*'\n",
    "args['eval_data_pattern']='eval_data/atl_june_tfr*'\n",
    "args['file_format']='tfr'\n",
    "\n",
    "# train and eval parameters\n",
    "args['train_batch_size']=500\n",
    "args['eval_batch_size']=1024\n",
    "args['max_train_steps']=40000\n",
    "args['eval_steps']=10\n",
    "\n",
    "# Execution parameters\n",
    "args['reader_num_threads']=16\n",
    "args['parser_num_threads']=16\n",
    "args['prefetch_buffer_size']=10000\n",
    "args['shuffle_buffer_size']=10000\n",
    "args['save_checkpoints_steps']=5000\n",
    "args['log_step_count_steps']=200\n",
    "args['throttle_secs']=30\n",
    "args['distribute']=False\n",
    "args['sloppy_ordering']=True\n",
    "args['save_summary_steps']=100\n",
    "\n",
    "# Model parameters\n",
    "args['learning_rate']=1e-3\n",
    "args['hypothesis']='linear'\n",
    "args['optimizer']='sgd'\n",
    "\n",
    "args = join_paths(args)\n",
    "\n",
    "model_dir = args['model_dir']\n",
    "print(\"using directory %s to store the model.\" % model_dir)\n",
    "print(\"Remove the directory if you want to start from scratch\".format(model_dir))\n",
    "_ = !gsutil -m rm -rf $model_dir\n",
    "\n",
    "print(\"=====================================================================================================\")\n",
    "from tools import create_runpy\n",
    "create_runpy(\"run_task.sh\", args)\n",
    "!cat ./run_task.sh\n",
    "print(\"=====================================================================================================\")\n",
    "print()\n",
    "\n",
    "# if you want to keep your jupyter notebook clean, rather use run_task.sh from a terminal \n",
    "#res = train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created the bash runner script named **```run_task.sh```**. \n",
    "\n",
    "Execute\n",
    "\n",
    "``` \n",
    "bash run_task.sh\n",
    "\n",
    "``` \n",
    "\n",
    "to perform the training from a shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_local_trainer(filename, args):\n",
    "    export = \"export PYTHONPATH=${PYTHONPATH}:${PWD}\\n\"\n",
    "    cmd = \"gcloud ml-engine local train --module-name train.task --package-path . -- \\\\\\n\"\n",
    "    _args = \"\".join(['  --{}=\"{}\"  \\\\\\n'.format(key, value) for key, value in args.items() if value != False])\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(export+cmd+_args)\n",
    "    \n",
    "def create_submit_job(filename, args):\n",
    "    _args = \"\".join(['    --{}=\"{}\"  \\\\\\n'.format(key, value) for key, value in args.items() if value != False])\n",
    "    TFVERSION=1.8\n",
    "    BUCKET='going-tfx'\n",
    "    JOBDIR=\"gs://{}/jobs\".format(BUCKET)\n",
    "    PACKAGE=\"train\"\n",
    "    REGION=\"us-east1\"\n",
    "    cmd=\"\"\"\n",
    "    JOBNAME=homeintime_$(date -u +%y%m%d_%H%M%S)\n",
    "    gcloud ml-engine jobs submit training $JOBNAME \\\\\n",
    "    --region={} \\\\\n",
    "    --module-name=train.task \\\\\n",
    "    --package-path={} \\\\\n",
    "    --job-dir={} \\\\\n",
    "    --staging-bucket=gs://going-tfx \\\\\n",
    "    --scale-tier=STANDARD_1 \\\\\n",
    "    --runtime-version={} \\\\\n",
    "    -- \\\\\\n{}\n",
    "    \"\"\".format(REGION, PACKAGE, JOBDIR, TFVERSION, _args)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(cmd)\n",
    "    return cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    JOBNAME=homeintime_$(date -u +%y%m%d_%H%M%S)\n",
      "    gcloud ml-engine jobs submit training $JOBNAME \\\n",
      "    --region=us-east1 \\\n",
      "    --module-name=train.task \\\n",
      "    --package-path=train \\\n",
      "    --job-dir=gs://going-tfx/jobs \\\n",
      "    --staging-bucket=gs://going-tfx \\\n",
      "    --scale-tier=STANDARD_1 \\\n",
      "    --runtime-version=1.8 \\\n",
      "    -- \\\n",
      "    --eval_steps=\"10\"  \\\n",
      "    --parser_num_threads=\"16\"  \\\n",
      "    --eval_data_pattern=\"gs://going-tfx/samples/eval_data/atl_june_tfr*\"  \\\n",
      "    --train_batch_size=\"256\"  \\\n",
      "    --shuffle_buffer_size=\"10000\"  \\\n",
      "    --eval_batch_size=\"1024\"  \\\n",
      "    --sloppy_ordering=\"True\"  \\\n",
      "    --reader_num_threads=\"16\"  \\\n",
      "    --file_format=\"tfr\"  \\\n",
      "    --log_step_count_steps=\"200\"  \\\n",
      "    --model_dir=\"gs://going-tfx/samples/model\"  \\\n",
      "    --throttle_secs=\"30\"  \\\n",
      "    --optimizer=\"sgd\"  \\\n",
      "    --learning_rate=\"0.001\"  \\\n",
      "    --hypothesis=\"linear\"  \\\n",
      "    --save_summary_steps=\"100\"  \\\n",
      "    --max_train_steps=\"8000\"  \\\n",
      "    --prefetch_buffer_size=\"10000\"  \\\n",
      "    --metadata_dir=\"gs://going-tfx/samples/metadata\"  \\\n",
      "    --train_data_pattern=\"gs://going-tfx/samples/train_data/atl_june_tfr*\"  \\\n",
      "    --save_checkpoints_steps=\"2000\"  \\\n",
      "    --base_dir=\"gs://going-tfx/samples\"  \\\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "cmd = create_submit_job(\"submit_job.sh\", args)\n",
    "print(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export PYTHONPATH=${PYTHONPATH}:${PWD}\n",
      "gcloud ml-engine local train --module-name train.task --package-path . -- \\\n",
      "  --eval_steps=\"10\"  \\\n",
      "  --parser_num_threads=\"16\"  \\\n",
      "  --eval_data_pattern=\"gs://going-tfx/samples/eval_data/atl_june_tfr*\"  \\\n",
      "  --train_batch_size=\"256\"  \\\n",
      "  --shuffle_buffer_size=\"10000\"  \\\n",
      "  --eval_batch_size=\"1024\"  \\\n",
      "  --sloppy_ordering=\"True\"  \\\n",
      "  --reader_num_threads=\"16\"  \\\n",
      "  --file_format=\"tfr\"  \\\n",
      "  --log_step_count_steps=\"200\"  \\\n",
      "  --model_dir=\"gs://going-tfx/samples/model\"  \\\n",
      "  --throttle_secs=\"30\"  \\\n",
      "  --optimizer=\"adam\"  \\\n",
      "  --learning_rate=\"0.001\"  \\\n",
      "  --hypothesis=\"linear\"  \\\n",
      "  --save_summary_steps=\"100\"  \\\n",
      "  --max_train_steps=\"8000\"  \\\n",
      "  --prefetch_buffer_size=\"10000\"  \\\n",
      "  --metadata_dir=\"gs://going-tfx/samples/metadata\"  \\\n",
      "  --train_data_pattern=\"gs://going-tfx/samples/train_data/atl_june_tfr*\"  \\\n",
      "  --save_checkpoints_steps=\"2000\"  \\\n",
      "  --base_dir=\"gs://going-tfx/samples\"  \\\n"
     ]
    }
   ],
   "source": [
    "!cat train_local.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
