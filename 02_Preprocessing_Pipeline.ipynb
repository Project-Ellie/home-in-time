{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE=\"./prep\"\n",
    "from tools import make_src_dumper\n",
    "write_py = make_src_dumper(PACKAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Preprocessing data for ML with tensorflow_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Signature data in Bigquery\n",
    "We collected the raw data that we use from various sources into a single denormalized table holding the data in so-called signature format. That table's schema is meant to reflect the structure of the data/requests that we expect to be served at prediction time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>298329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_records\n",
       "0       298329"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery num_records\n",
    "select count(*) as num_records FROM `going-tfx.examples.ATL_JUNE_SIGNATURE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DEP_DOW</th>\n",
       "      <th>AIRLINE_NAME</th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>DEP_T</th>\n",
       "      <th>DEP</th>\n",
       "      <th>DEP_LAT</th>\n",
       "      <th>...</th>\n",
       "      <th>WND_SPD_DEP</th>\n",
       "      <th>ARR_T</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>ARR</th>\n",
       "      <th>ARR_LAT</th>\n",
       "      <th>ARR_LON</th>\n",
       "      <th>ARR_W</th>\n",
       "      <th>MEAN_TEMP_ARR</th>\n",
       "      <th>MEAN_VIS_ARR</th>\n",
       "      <th>WND_SPD_ARR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-06-01</td>\n",
       "      <td>2002</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>US Airways Inc.: US (Merged with America West ...</td>\n",
       "      <td>US</td>\n",
       "      <td>610</td>\n",
       "      <td>ATL</td>\n",
       "      <td>33.63</td>\n",
       "      <td>...</td>\n",
       "      <td>6.9</td>\n",
       "      <td>712</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>CLT</td>\n",
       "      <td>35.21</td>\n",
       "      <td>-80.94</td>\n",
       "      <td>CHARLOTTE DOUGLAS MUNICIPAL A</td>\n",
       "      <td>78.3</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002-06-01</td>\n",
       "      <td>2002</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Delta Air Lines Inc.: DL</td>\n",
       "      <td>DL</td>\n",
       "      <td>620</td>\n",
       "      <td>ATL</td>\n",
       "      <td>33.63</td>\n",
       "      <td>...</td>\n",
       "      <td>6.9</td>\n",
       "      <td>740</td>\n",
       "      <td>9.0</td>\n",
       "      <td>MCO</td>\n",
       "      <td>28.42</td>\n",
       "      <td>-81.30</td>\n",
       "      <td>ORLANDO INTERNATIONAL AIRPORT</td>\n",
       "      <td>77.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002-06-01</td>\n",
       "      <td>2002</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Delta Air Lines Inc.: DL</td>\n",
       "      <td>DL</td>\n",
       "      <td>620</td>\n",
       "      <td>ATL</td>\n",
       "      <td>33.63</td>\n",
       "      <td>...</td>\n",
       "      <td>6.9</td>\n",
       "      <td>738</td>\n",
       "      <td>55.0</td>\n",
       "      <td>TPA</td>\n",
       "      <td>27.97</td>\n",
       "      <td>-82.53</td>\n",
       "      <td>TAMPA INTERNATIONAL AIRPORT</td>\n",
       "      <td>79.1</td>\n",
       "      <td>9.9</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE  YEAR  MONTH  DAY  DEP_DOW  \\\n",
       "0  2002-06-01  2002      6    1        7   \n",
       "1  2002-06-01  2002      6    1        7   \n",
       "2  2002-06-01  2002      6    1        7   \n",
       "\n",
       "                                        AIRLINE_NAME AIRLINE  DEP_T  DEP  \\\n",
       "0  US Airways Inc.: US (Merged with America West ...      US    610  ATL   \n",
       "1                           Delta Air Lines Inc.: DL      DL    620  ATL   \n",
       "2                           Delta Air Lines Inc.: DL      DL    620  ATL   \n",
       "\n",
       "   DEP_LAT     ...       WND_SPD_DEP  ARR_T ARR_DELAY  ARR  ARR_LAT  ARR_LON  \\\n",
       "0    33.63     ...               6.9    712     -12.0  CLT    35.21   -80.94   \n",
       "1    33.63     ...               6.9    740       9.0  MCO    28.42   -81.30   \n",
       "2    33.63     ...               6.9    738      55.0  TPA    27.97   -82.53   \n",
       "\n",
       "                           ARR_W  MEAN_TEMP_ARR MEAN_VIS_ARR  WND_SPD_ARR  \n",
       "0  CHARLOTTE DOUGLAS MUNICIPAL A           78.3          9.5          2.7  \n",
       "1  ORLANDO INTERNATIONAL AIRPORT           77.4          9.6          5.7  \n",
       "2    TAMPA INTERNATIONAL AIRPORT           79.1          9.9          5.5  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery sample\n",
    "select * FROM `going-tfx.examples.ATL_JUNE_SIGNATURE` limit 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying reproducible random subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample_queries written to ./prep/sample_queries.py.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_queries(columns, fractions=[80,10,10], rate=0.1):\n",
    "    \"\"\"Reproducible random sets of flights from Atlanta in any month of June.\n",
    "\n",
    "    Requires Bigquery table ATL_JUNE_SIGNATURE to be present. This function is meant\n",
    "    to encapsulate the aspect of reproducible random sets from the rest of the code.\n",
    "    \n",
    "    Creates three statements (strings), each of which will produce a non-intersecting\n",
    "    random subset of the data in ATL_JUNE_SIGNATURE table. Will return a map with keys\n",
    "    'train', 'eval', and 'test' to hold those statements. \n",
    "    \n",
    "    Args:\n",
    "        columns: A string holding a comma-separated list of column names to fetch.\n",
    "        fractions: an array of exactly three doubles defining the precise splits.\n",
    "        rate: an additional parameter determining the overall sample rate. 0.1 means 10% is\n",
    "        being split into the three given fractions.\n",
    "        \n",
    "    Returns:\n",
    "        a dict like {'train': stmt1, 'eval': stmt2, 'test': stmt3}, where stmt1, stmt2, stmt3\n",
    "        are three strings that represent valid statements against bigquery.\n",
    "    \"\"\"\n",
    "    \n",
    "    def sample_query(columns, total, lower, upper):\n",
    "        col_string=\", \".join(columns)\n",
    "        return \"\"\"\n",
    "        SELECT\n",
    "            {0}\n",
    "        FROM \n",
    "            `going-tfx.examples.ATL_JUNE_SIGNATURE` \n",
    "        where\n",
    "            MOD(ABS(FARM_FINGERPRINT(\n",
    "                CONCAT(DATE,AIRLINE,ARR)\n",
    "            )) + DEP_T, {1}) >= {2} \n",
    "        and\n",
    "            MOD(ABS(FARM_FINGERPRINT(\n",
    "                CONCAT( DATE, AIRLINE, ARR)\n",
    "            )) + DEP_T, {1}) < {3} \n",
    "        \"\"\".format(col_string, total, lower, upper)\n",
    "    \n",
    "    start = 0\n",
    "    total = int(sum(fractions) / rate)\n",
    "    res = []\n",
    "    for f in fractions:\n",
    "        f_ = int(f) \n",
    "        q = sample_query(columns, total, start, start+f_)\n",
    "        start = start + f_\n",
    "        res.append(q)\n",
    "    return dict(zip(['train', 'eval', 'test'], res))\n",
    "write_py(sample_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying ```sample_queries()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the function's behaviour: from the where clause of the 'train' query you can tell that it reproducibly chooses those records that come with a hash value modulo between $0$ and $90$ out of $10000$ different values. That's $90\\%$ of $1\\%$, just as we specified in the call to ```sample_queries```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code specific to this signature structure is kept in a single file called ```model_config.py``` located in the train package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        SELECT\n",
      "            DEP_DOW, DEP_T, DEP_LAT, DEP_LON, DEP_DELAY, MEAN_TEMP_DEP, MEAN_VIS_DEP, WND_SPD_DEP, ARR_LAT, ARR_LON, ARR_DELAY, MEAN_TEMP_ARR, MEAN_VIS_ARR, WND_SPD_ARR, ARR, AIRLINE\n",
      "        FROM \n",
      "            `going-tfx.examples.ATL_JUNE_SIGNATURE` \n",
      "        where\n",
      "            MOD(ABS(FARM_FINGERPRINT(\n",
      "                CONCAT(DATE,AIRLINE,ARR)\n",
      "            )) + DEP_T, 10000) >= 0 \n",
      "        and\n",
      "            MOD(ABS(FARM_FINGERPRINT(\n",
      "                CONCAT( DATE, AIRLINE, ARR)\n",
      "            )) + DEP_T, 10000) < 90 \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from train.model_config import SIGNATURE_COLUMNS\n",
    "queries = sample_queries(SIGNATURE_COLUMNS, [90,5,5], .01)\n",
    "print(queries['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = num_records['num_records'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 2661 = 0.891968263226% of all examples. Showing first three:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEP_DOW</th>\n",
       "      <th>DEP_T</th>\n",
       "      <th>DEP_LAT</th>\n",
       "      <th>DEP_LON</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>MEAN_TEMP_DEP</th>\n",
       "      <th>MEAN_VIS_DEP</th>\n",
       "      <th>WND_SPD_DEP</th>\n",
       "      <th>ARR_LAT</th>\n",
       "      <th>ARR_LON</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>MEAN_TEMP_ARR</th>\n",
       "      <th>MEAN_VIS_ARR</th>\n",
       "      <th>WND_SPD_ARR</th>\n",
       "      <th>ARR</th>\n",
       "      <th>AIRLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>1125</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>13.0</td>\n",
       "      <td>78.1</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>36.19</td>\n",
       "      <td>-95.88</td>\n",
       "      <td>11.0</td>\n",
       "      <td>80.3</td>\n",
       "      <td>9.9</td>\n",
       "      <td>7.8</td>\n",
       "      <td>TUL</td>\n",
       "      <td>DL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1540</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>5.0</td>\n",
       "      <td>78.1</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>40.63</td>\n",
       "      <td>-73.77</td>\n",
       "      <td>16.0</td>\n",
       "      <td>73.8</td>\n",
       "      <td>9.2</td>\n",
       "      <td>11.4</td>\n",
       "      <td>JFK</td>\n",
       "      <td>DL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>1630</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>31.0</td>\n",
       "      <td>78.1</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>30.53</td>\n",
       "      <td>-91.15</td>\n",
       "      <td>20.0</td>\n",
       "      <td>77.2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2.2</td>\n",
       "      <td>BTR</td>\n",
       "      <td>DL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEP_DOW  DEP_T  DEP_LAT  DEP_LON  DEP_DELAY  MEAN_TEMP_DEP  MEAN_VIS_DEP  \\\n",
       "0        7   1125    33.63   -84.42       13.0           78.1           9.6   \n",
       "1        7   1540    33.63   -84.42        5.0           78.1           9.6   \n",
       "2        7   1630    33.63   -84.42       31.0           78.1           9.6   \n",
       "\n",
       "   WND_SPD_DEP  ARR_LAT  ARR_LON  ARR_DELAY  MEAN_TEMP_ARR  MEAN_VIS_ARR  \\\n",
       "0          6.9    36.19   -95.88       11.0           80.3           9.9   \n",
       "1          6.9    40.63   -73.77       16.0           73.8           9.2   \n",
       "2          6.9    30.53   -91.15       20.0           77.2           8.5   \n",
       "\n",
       "   WND_SPD_ARR  ARR AIRLINE  \n",
       "0          7.8  TUL      DL  \n",
       "1         11.4  JFK      DL  \n",
       "2          2.2  BTR      DL  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.datalab.bigquery as dlbq\n",
    "df = dlbq.Query(queries['train']).execute().result().to_dataframe()\n",
    "print('Only {} = {}% of all examples. Showing first three:'.format(len(df), len(df)/num_records*100))\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. The Pre-processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pre_process written to ./prep/pre_process.py.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pre_process(row):\n",
    "    import tensorflow_transform as tft\n",
    "    from tools import tf_haversine\n",
    "\n",
    "    def add_engineered(row):\n",
    "        dep_lat = row['DEP_LAT']\n",
    "        dep_lon = row['DEP_LON']\n",
    "        arr_lat = row['ARR_LAT']\n",
    "        arr_lon = row['ARR_LON']\n",
    "\n",
    "        row['DEP_HOD'] = row['DEP_T'] // 100\n",
    "        row.pop('DEP_T')  # no longer needed\n",
    "\n",
    "        row['DIFF_LAT'] = arr_lat - dep_lat\n",
    "        row['DIFF_LON'] = arr_lon - dep_lon\n",
    "        row['DISTANCE'] = tf_haversine(arr_lat, arr_lon, dep_lat, dep_lon)\n",
    "        return row\n",
    "\n",
    "    def scale_floats(row):\n",
    "        for c in ['MEAN_TEMP_DEP', 'MEAN_VIS_DEP', 'WND_SPD_DEP', 'MEAN_TEMP_ARR', 'MEAN_VIS_ARR', 'WND_SPD_ARR', 'DEP_DELAY',\n",
    "                 'DIFF_LAT', 'DIFF_LON', 'DISTANCE']:\n",
    "            row[c] = tft.scale_to_0_1(row[c])\n",
    "        return row\n",
    "\n",
    "    def categorical_from_strings(row):\n",
    "        row['AIRLINE'] = tft.string_to_int(row['AIRLINE'])\n",
    "        row['ARR'] = tft.string_to_int(row['ARR'])\n",
    "        return row\n",
    "    \n",
    "    row = row.copy()\n",
    "    row = add_engineered(row)\n",
    "    row = scale_floats(row)\n",
    "    row = categorical_from_strings(row)\n",
    "    return row\n",
    "write_py(pre_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, let's now verify the pre-processing function. We're aware that putting the basic transformations into a wrapper function does not make it easily testable. But please consider that the all basic transformations are somewhat trivial and those with the ```tft``` functions inside are by their nature not trivial to test anyway. Btw: Another interesting question is: Does this wrapping impact performance? Honestly, I don't know for sure - that may become subject to a different tutorial. \n",
    "\n",
    "Now here's how we test our code: We create a super-small pipeline with five records and verify the output of the analyse-and-transform lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying ```pre_process()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'WND_SPD_DEP': 6.9, u'DEP_DELAY': 13.0, u'ARR_LAT': 36.19, u'WND_SPD_ARR': 7.8, u'MEAN_VIS_DEP': 9.6, u'DEP_T': 1125, u'MEAN_TEMP_ARR': 80.3, u'DEP_LON': -84.42, u'DEP_DOW': 7, u'MEAN_VIS_ARR': 9.9, u'ARR': 'TUL', u'AIRLINE': 'DL', u'MEAN_TEMP_DEP': 78.1, u'ARR_DELAY': 11.0, u'DEP_LAT': 33.63, u'ARR_LON': -95.88}, {u'WND_SPD_DEP': 6.9, u'DEP_DELAY': 5.0, u'ARR_LAT': 40.63, u'WND_SPD_ARR': 11.4, u'MEAN_VIS_DEP': 9.6, u'DEP_T': 1540, u'MEAN_TEMP_ARR': 73.8, u'DEP_LON': -84.42, u'DEP_DOW': 7, u'MEAN_VIS_ARR': 9.2, u'ARR': 'JFK', u'AIRLINE': 'DL', u'MEAN_TEMP_DEP': 78.1, u'ARR_DELAY': 16.0, u'DEP_LAT': 33.63, u'ARR_LON': -73.77}, {u'WND_SPD_DEP': 6.9, u'DEP_DELAY': 31.0, u'ARR_LAT': 30.53, u'WND_SPD_ARR': 2.2, u'MEAN_VIS_DEP': 9.6, u'DEP_T': 1630, u'MEAN_TEMP_ARR': 77.2, u'DEP_LON': -84.42, u'DEP_DOW': 7, u'MEAN_VIS_ARR': 8.5, u'ARR': 'BTR', u'AIRLINE': 'DL', u'MEAN_TEMP_DEP': 78.1, u'ARR_DELAY': 20.0, u'DEP_LAT': 33.63, u'ARR_LON': -91.15}, {u'WND_SPD_DEP': 7.4, u'DEP_DELAY': -1.0, u'ARR_LAT': 27.97, u'WND_SPD_ARR': 4.7, u'MEAN_VIS_DEP': 9.7, u'DEP_T': 620, u'MEAN_TEMP_ARR': 82.6, u'DEP_LON': -84.42, u'DEP_DOW': 1, u'MEAN_VIS_ARR': 9.9, u'ARR': 'TPA', u'AIRLINE': 'DL', u'MEAN_TEMP_DEP': 80.2, u'ARR_DELAY': -4.0, u'DEP_LAT': 33.63, u'ARR_LON': -82.53}, {u'WND_SPD_DEP': 7.4, u'DEP_DELAY': -4.0, u'ARR_LAT': 39.29, u'WND_SPD_ARR': 11.3, u'MEAN_VIS_DEP': 9.7, u'DEP_T': 825, u'MEAN_TEMP_ARR': 80.0, u'DEP_LON': -84.42, u'DEP_DOW': 1, u'MEAN_VIS_ARR': 9.4, u'ARR': 'MCI', u'AIRLINE': 'DL', u'MEAN_TEMP_DEP': 80.2, u'ARR_DELAY': -1.0, u'DEP_LAT': 33.63, u'ARR_LON': -94.71}]\n"
     ]
    }
   ],
   "source": [
    "first5=df.head().to_dict(orient='records')\n",
    "print(first5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-35-044ea862cba8>:26: string_to_int (from tensorflow_transform.mappers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tft.compute_and_apply_vocabulary()` instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /tmp/tftransform_tmp/9afd2251e7d34c6d8957c26a3ff4c46d/saved_model.pb\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /tmp/tftransform_tmp/43644c9114fd47c6b2f16be54969495d/saved_model.pb\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n",
      "WARNING:root:Deleting 1 existing files in target path matching: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tftransform_tmp/23df1c4933e046dc8f5ecc6375d51970/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tftransform_tmp/23df1c4933e046dc8f5ecc6375d51970/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tftransform_tmp/23df1c4933e046dc8f5ecc6375d51970/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tftransform_tmp/23df1c4933e046dc8f5ecc6375d51970/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\t\\n\\007Const:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\t\\n\\007Const:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\t\\n\\007Const:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\t\\n\\007Const:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "from train.model_config import (SIGNATURE_COLUMNS, TRAINING_COLUMNS,\n",
    "    TRAINING_METADATA, SIGNATURE_METADATA, ORDERED_TRAINING_COLUMNS)\n",
    "import tensorflow_transform.beam.impl as beam_impl\n",
    "import apache_beam as beam\n",
    "\n",
    "signature_dataset = (first5, SIGNATURE_METADATA)\n",
    "\n",
    "with beam_impl.Context(temp_dir='/tmp'):\n",
    "    tds, transform_fn = (signature_dataset | beam_impl.AnalyzeAndTransformDataset(pre_process))\n",
    "    t_data, t_metadata = tds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Please confirm for yourself that some of the features now scale from $0$ to $1$, and the differences in longitude and latitude ```DIFF_LAT``` and ```DIFF_LON``` have been added to the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'ARR_LAT': 36.19, u'DISTANCE': 0.7537452, u'WND_SPD_DEP': 0.0, u'DEP_DELAY': 0.4857143, u'DEP_HOD': 11, u'WND_SPD_ARR': 0.6086957, u'MEAN_VIS_DEP': 0.0, u'DIFF_LAT': 0.64928895, u'MEAN_TEMP_ARR': 0.73863673, u'DEP_LON': -84.42, u'DEP_DOW': 7, u'MEAN_VIS_ARR': 1.0, u'ARR': 0, u'AIRLINE': 0, u'MEAN_TEMP_DEP': 0.0, u'ARR_DELAY': 11.0, u'DEP_LAT': 33.63, u'ARR_LON': -95.88, u'DIFF_LON': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(t_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the 5 different longitudinal differences are scaled between $0$ and $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.05291715, 0.21393016, 0.6037991, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(sorted([r['DIFF_LON'] for r in t_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. The Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exec_pipeline_prod written to ./prep/exec_pipeline_prod.py.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exec_pipeline_prod (options, train_dir, eval_dir, test_dir, \n",
    "                        metadata_dir, tmp_dir, \n",
    "                        fractions, sample_rate, prefix, \n",
    "                        encode='tfrecord', \n",
    "                        runner='DirectRunner'):\n",
    "    \n",
    "    import os\n",
    "    import tensorflow_transform as tft\n",
    "    import tensorflow_transform.beam.impl as beam_impl\n",
    "    import apache_beam as beam\n",
    "    from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "    from tensorflow_transform.tf_metadata import dataset_schema\n",
    "    from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "    \n",
    "    from train.model_config import (SIGNATURE_COLUMNS, TRAINING_COLUMNS,\n",
    "        TRAINING_METADATA, SIGNATURE_METADATA, ORDERED_TRAINING_COLUMNS)\n",
    "    from prep.pre_process import pre_process\n",
    "    from prep.sample_queries import sample_queries\n",
    "\n",
    "    with beam.Pipeline(runner, options=options) as p:\n",
    "        with beam_impl.Context(temp_dir=tmp_dir):\n",
    "\n",
    "            def write_to_files(data, prefix, phase):\n",
    "                tfr_encoder = tft.coders.ExampleProtoCoder(t_metadata.schema)            \n",
    "                if encode in ['tfrecord', 'both', None]:\n",
    "                    _ = (data\n",
    "                        | ('EncodeTFRecord_' + phase) >> beam.Map(tfr_encoder.encode)\n",
    "                        | ('WriteTFRecord_' + phase) >> beam.io.WriteToTFRecord(prefix+'_tfr'))\n",
    "\n",
    "                if encode in ['csv', 'both', None]:\n",
    "                    csv_encoder = tft.coders.CsvCoder(ORDERED_TRAINING_COLUMNS, TRAINING_METADATA.schema)    \n",
    "                    _ = (data \n",
    "                        | ('EncodeCSV_train' + phase) >> beam.Map(csv_encoder.encode)\n",
    "                        | ('WriteText_train' + phase) >> beam.io.WriteToText(file_path_prefix=prefix+'_csv'))\n",
    "        \n",
    "            # Process training data and obtain transform_fn\n",
    "            #\n",
    "            queries = sample_queries(SIGNATURE_COLUMNS, fractions, sample_rate)\n",
    "\n",
    "            signature_data = (p | \"ReadFromBigQuery_train\"  \n",
    "                              >> beam.io.Read(beam.io.BigQuerySource(\n",
    "                                  query=queries['train'], use_standard_sql=True)))\n",
    "            signature_dataset = (signature_data, SIGNATURE_METADATA)\n",
    "            \n",
    "            tds, transform_fn = (signature_dataset | \"AnalyzeAndTransform\" \n",
    "                        >> beam_impl.AnalyzeAndTransformDataset(pre_process))\n",
    "            t_data, t_metadata = tds\n",
    "\n",
    "            train_prefix = os.path.join(train_dir, prefix)\n",
    "            write_to_files(t_data, train_prefix, 'train')\n",
    "            \n",
    "            #  Process evaluation data with the obtained transform_fn\n",
    "            #\n",
    "            signature_data = (p | \"ReadFromBigQuery_eval\"  \n",
    "                              >> beam.io.Read(beam.io.BigQuerySource(\n",
    "                                  query=queries['eval'], use_standard_sql=True))) \n",
    "            signature_dataset = (signature_data, SIGNATURE_METADATA)\n",
    "\n",
    "            t_dataset = ((signature_dataset, transform_fn) \n",
    "                         | \"TransformEval\" >> beam_impl.TransformDataset())\n",
    "            t_data, t_metadata = t_dataset\n",
    "\n",
    "            eval_prefix = os.path.join(eval_dir, prefix)\n",
    "            write_to_files(t_data, eval_prefix, 'eval')\n",
    "\n",
    "            #  Also process test data with the obtained transform_fn\n",
    "            #\n",
    "            signature_data = (p | \"ReadFromBigQuery_test\"  \n",
    "                              >> beam.io.Read(beam.io.BigQuerySource(\n",
    "                                  query=queries['test'], use_standard_sql=True)))\n",
    "            signature_dataset = (signature_data, SIGNATURE_METADATA)\n",
    "\n",
    "            t_dataset = ((signature_dataset, transform_fn) \n",
    "                         | \"TransformTest\" >> beam_impl.TransformDataset())\n",
    "            t_data, t_metadata = t_dataset           \n",
    "\n",
    "            test_prefix = os.path.join(test_dir, prefix)\n",
    "            write_to_files(t_data, test_prefix, 'text')\n",
    "            \n",
    "            # save transforma function to disk for use at serving time\n",
    "            #\n",
    "            transform_fn | 'WriteTransformFn' >> transform_fn_io.WriteTransformFn(metadata_dir)\n",
    "\n",
    "write_py(exec_pipeline_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run_job written to ./prep/run_job.py.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_job(args):\n",
    "    \n",
    "    import datetime\n",
    "    import apache_beam as beam\n",
    "    from prep.exec_pipeline_prod import exec_pipeline_prod\n",
    "    \n",
    "    job_name = 'tft-tutorial' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')    \n",
    "    \n",
    "    options = {\n",
    "        'staging_location': args['stage_dir'],\n",
    "        'temp_location': args['tmp_dir'],\n",
    "        'job_name': job_name,\n",
    "        'project': args['project'],\n",
    "        'max_num_workers': int(args['max_workers']),\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True,\n",
    "        'requirements_file': 'dataflow_requirements.txt'\n",
    "    }    \n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "\n",
    "    fractions = [int(n) for n in args['fractions'].split(\",\")]\n",
    "\n",
    "    exec_pipeline_prod (opts, args['train_dir'], args['eval_dir'],args['test_dir'],\n",
    "                        args['metadata_dir'], args['tmp_dir'],\n",
    "                        fractions, float(args['sample_rate']), args['prefix'],\n",
    "                        encode=args['encode'], runner=args['runner'])\n",
    "write_py(run_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Verifying the production pipeline\n",
    "This little helper function cleans all sub-directories in a particular sub-project directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(subproject):\n",
    "    import os\n",
    "    basedir = os.path.join('gs://going-tfx/', subproject)\n",
    "\n",
    "    for d in ['train_data/*', 'eval_data/*', 'test_data/*', 'tmp/*', 'model/*', 'metadata/*']:\n",
    "        target = os.path.join(basedir, d)\n",
    "        !echo gsutil -m rm -rf $target\n",
    "        _ = !gsutil -m rm -rf $target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_ROOT='experimental'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsutil -m rm -rf gs://going-tfx/experimental/train_data/*\n",
      "gsutil -m rm -rf gs://going-tfx/experimental/eval_data/*\n",
      "gsutil -m rm -rf gs://going-tfx/experimental/test_data/*\n",
      "gsutil -m rm -rf gs://going-tfx/experimental/tmp/*\n",
      "gsutil -m rm -rf gs://going-tfx/experimental/model/*\n",
      "gsutil -m rm -rf gs://going-tfx/experimental/metadata/*\n"
     ]
    }
   ],
   "source": [
    "cleanup(WORK_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a local job on a small subset to verify everything is working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Dataset going-tfx:temp_dataset_6d6f725bfc944abf9283e584b71f979e does not exist so we will create it as temporary with location=US\n",
      "WARNING:root:Dataset going-tfx:temp_dataset_e22de90fbbd74bd487ee531818025382 does not exist so we will create it as temporary with location=US\n",
      "WARNING:root:Dataset going-tfx:temp_dataset_619ebdf2454247eca6c9ff2d7e7763f6 does not exist so we will create it as temporary with location=US\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_8:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\014\\n\\nConst_13:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from prep.prep_tools import join_paths\n",
    "\n",
    "args={}\n",
    "args['base_dir'] = \"gs://going-tfx/{}\".format(WORK_ROOT)\n",
    "args['train_dir'] = 'train_data'\n",
    "args['eval_dir'] = 'eval_data'\n",
    "args['test_dir'] = 'test_data'\n",
    "args['metadata_dir'] = 'metadata'\n",
    "args['stage_dir'] = 'staging'\n",
    "args['tmp_dir'] = 'tmp'\n",
    "args['project'] = 'going-tfx'\n",
    "args['prefix'] = 'atl_june'\n",
    "args['fractions'] = '80,10,10'\n",
    "args['sample_rate'] = 0.01\n",
    "args['max_workers'] = 1\n",
    "args['runner'] = 'DirectRunner'\n",
    "args['encode'] = 'both'\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "run_job(join_paths(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "A quick sanity check: Fetch one file from gs storage and have a sneak preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_from_gs(gsglob):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from train.model_config import ORDERED_TRAINING_COLUMNS\n",
    "\n",
    "    a_training_file = !gsutil ls $gsglob\n",
    "    a_training_file = a_training_file[0]\n",
    "    TEMP_DIR='/tmp/atl_june/{}'.format(WORK_ROOT)\n",
    "    !mkdir -p $TEMP_DIR\n",
    "    !gsutil cp $a_training_file $TEMP_DIR\n",
    "    a_training_file = !ls $TEMP_DIR\n",
    "    a_training_file = os.path.join(TEMP_DIR,a_training_file[0])\n",
    "    res=!wc -l $a_training_file\n",
    "    res=res[0].split(\" \")\n",
    "    print()\n",
    "    print(\"{} records in {}\".format(res[0], res[1]))\n",
    "    return pd.read_csv(a_training_file, names=ORDERED_TRAINING_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://going-tfx/experimental/train_data/atl_june_csv-00000-of-00003...\n",
      "/ [1 files][142.6 KiB/142.6 KiB]                                                \n",
      "Operation completed over 1 objects/142.6 KiB.                                    \n",
      "\n",
      "1000 records in /tmp/atl_june/experimental/atl_june_csv-00000-of-00003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>ARR</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>ARR_LAT</th>\n",
       "      <th>ARR_LON</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>DEP_DOW</th>\n",
       "      <th>DEP_HOD</th>\n",
       "      <th>DEP_LAT</th>\n",
       "      <th>DEP_LON</th>\n",
       "      <th>DIFF_LAT</th>\n",
       "      <th>DIFF_LON</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>MEAN_TEMP_ARR</th>\n",
       "      <th>MEAN_TEMP_DEP</th>\n",
       "      <th>MEAN_VIS_ARR</th>\n",
       "      <th>MEAN_VIS_DEP</th>\n",
       "      <th>WND_SPD_ARR</th>\n",
       "      <th>WND_SPD_DEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>53.0</td>\n",
       "      <td>35.43</td>\n",
       "      <td>-82.54</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.399160</td>\n",
       "      <td>0.810974</td>\n",
       "      <td>0.019242</td>\n",
       "      <td>0.380682</td>\n",
       "      <td>0.739496</td>\n",
       "      <td>0.211679</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.286822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>51.0</td>\n",
       "      <td>31.53</td>\n",
       "      <td>-84.19</td>\n",
       "      <td>0.135065</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>33.63</td>\n",
       "      <td>-84.42</td>\n",
       "      <td>0.308123</td>\n",
       "      <td>0.793222</td>\n",
       "      <td>0.015087</td>\n",
       "      <td>0.700758</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.532847</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.248062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     AIRLINE  ARR  ARR_DELAY  ARR_LAT  ARR_LON  DEP_DELAY  DEP_DOW  DEP_HOD  \\\n",
       "230        1   56       53.0    35.43   -82.54   0.103896        3        7   \n",
       "196        1   96       51.0    31.53   -84.19   0.135065        6        9   \n",
       "\n",
       "     DEP_LAT  DEP_LON  DIFF_LAT  DIFF_LON  DISTANCE  MEAN_TEMP_ARR  \\\n",
       "230    33.63   -84.42  0.399160  0.810974  0.019242       0.380682   \n",
       "196    33.63   -84.42  0.308123  0.793222  0.015087       0.700758   \n",
       "\n",
       "     MEAN_TEMP_DEP  MEAN_VIS_ARR  MEAN_VIS_DEP  WND_SPD_ARR  WND_SPD_DEP  \n",
       "230       0.739496      0.211679         1.000     0.001901     0.286822  \n",
       "196       0.970588      0.532847         0.625     0.002601     0.248062  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe = dataframe_from_gs('gs://going-tfx/%s/train_data/atl_june_csv-00000-of-*' % WORK_ROOT)\n",
    "probe.sample(frac=1.0)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Run Your Code in Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "[ -z \"$WORK_ROOT\" ] && echo set WORK_ROOT to a particular value of your choice and find your results in gs://going-tfx/WORK_ROOT && exit -1\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}\n",
    "python -m prep.task \\\n",
    "    --project=going-tfx \\\n",
    "    --base_dir=gs://going-tfx/$WORK_ROOT/ \\\n",
    "    --sample_rate=1.0 \\\n",
    "    --prefix=atl_june \\\n",
    "    --encode=tfrecord \\\n",
    "    --runner=DataflowRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Have a look at the files that now consitute the production-ready package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48\n",
      "-rw-r--r-- 1 wgiersche wgiersche 4067 Dec 27 20:42 exec_pipeline_prod.py\n",
      "-rw-r--r-- 1 wgiersche wgiersche    0 Nov 24 07:51 __init__.py\n",
      "-rw-r--r-- 1 wgiersche wgiersche 1114 Dec 27 20:26 pre_process.py\n",
      "-rw-r--r-- 1 wgiersche wgiersche  526 Nov 24 12:33 prep_tools.py\n",
      "-rw-r--r-- 1 wgiersche wgiersche 1034 Dec 27 20:47 run_job.py\n",
      "-rw-r--r-- 1 wgiersche wgiersche 1871 Dec 27 20:25 sample_queries.py\n",
      "-rw-r--r-- 1 wgiersche wgiersche 3233 Dec 27 18:57 task.py\n",
      "\n",
      "preprocess.py:\n",
      "[ -z \"$WORK_ROOT\" ] && echo set WORK_ROOT to a particular value of your choice and find your results in gs://going-tfx/WORK_ROOT && exit -1\n",
      "export PYTHONPATH=${PYTHONPATH}:${PWD}\n",
      "python -m prep.task \\\n",
      "    --project=going-tfx \\\n",
      "    --base_dir=gs://going-tfx/$WORK_ROOT/ \\\n",
      "    --sample_rate=1.0 \\\n",
      "    --prefix=atl_june \\\n",
      "    --encode=tfrecord \\\n",
      "    --runner=DataflowRunner"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -l ./prep | grep -v pyc\n",
    "echo\n",
    "echo preprocess.py:\n",
    "cat preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now we're ready to run the entire code as a script in production. All the functions that we developed in this notebook, have already been written into python files that make a complete package ```prep``` that we can send to Google's ```DataFlowRunner``` in the cloud.\n",
    "Set WORK_ROOT to your choice and execute \n",
    "\n",
    "```bash preprocess.py``` \n",
    "\n",
    "on a terminal. It's going to take up to 20 minutes, if sample_rate is 1.0. Observe the progress in Google Cloud Dataflow and after the job is finished verify that there are indeed directories containing training, evaluation and test datasets. We'll use those files in [the training notebook](./05_Training.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
