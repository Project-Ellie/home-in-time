{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE=\"./train\"\n",
    "from tools import make_src_dumper\n",
    "write_py = make_src_dumper(PACKAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ```tf.data``` Input Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These input functions read from any number of files containing pre-processed data\n",
    "In our parlour, this data is at the **training stage**. It's been fetched from a BigQuery table and pre-processed. It's not what we expect to come in at prediction time! In our [pre-processing pipeline](./02_Preprocessing_Pipeline.ipynb), we applied a transform function to the **signature stage** data. The reason is simply that we want to avoid having to do that transformation again and again during our model development.\n",
    "\n",
    "During the pre-processing, the pre-processing function has been stored in a metadata directory that is available to us to treat data at prediction time (**signature stage**) exactly the same way that our training data has been treated.\n",
    "\n",
    "We support reading from csv files, which is sometimes preferred in earlier exploration phases, because results in CSV files can more easily be inspected with standard tools. But we also support reading from tfrecord files, which is a format that is highly optimized for maximum performant distributed computing.\n",
    "\n",
    "Any of these functions can be passed to the estimator, such that the estimator can call it in its own session/graph context to create a particularly useful input tensor. That input tensor will return the next batch of input records whenever it is evaluated. \n",
    "\n",
    "```make_input_fns()``` returns a dict with those two input_functions. We chose this design so that we can choose the file type at any point in time, e.g. by supplying a respectivie command line parameter to the training script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make_input_fns written to ./train/make_input_fns.py.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_input_fns():\n",
    "    from train.make_csv_input_fn import make_csv_input_fn\n",
    "    from train.make_tfr_input_fn import make_tfr_input_fn\n",
    "    \n",
    "    return {\n",
    "        'csv': make_csv_input_fn,\n",
    "        'tfr': make_tfr_input_fn\n",
    "    }\n",
    "write_py(make_input_fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Read from CSV\n",
    "Not so good for production, but may come handy for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make_csv_input_fn written to ./train/make_csv_input_fn.py.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_csv_input_fn(filename_pattern, batch_size, options): \n",
    "\n",
    "    import tensorflow as tf\n",
    "    from train.model_config import ORDERED_TRAINING_DEFAULTS\n",
    "    from train.model_config import ORDERED_TRAINING_COLUMNS\n",
    "    from train.model_config import LABEL_COLUMN\n",
    "    \n",
    "    \n",
    "    def _input_fn():\n",
    "        filenames = tf.gfile.Glob(filename_pattern)\n",
    "        dataset = tf.data.TextLineDataset(filenames)\n",
    "\n",
    "        def decode_csv(row):\n",
    "            cols = tf.decode_csv(row, record_defaults=ORDERED_TRAINING_DEFAULTS)\n",
    "            features = dict(zip(ORDERED_TRAINING_COLUMNS, cols))\n",
    "            return features\n",
    "\n",
    "        def pop_target(features):\n",
    "            target = features.pop(LABEL_COLUMN)\n",
    "            return features, target\n",
    "        \n",
    "        if options['shuffle_buffer_size'] is not None:\n",
    "            dataset = dataset.shuffle(buffer_size=options['shuffle_buffer_size'])\n",
    "                \n",
    "        dataset = (dataset.repeat()\n",
    "                   .map(decode_csv)\n",
    "                   .map(pop_target)\n",
    "                   .batch(batch_size))\n",
    "        \n",
    "        if options['distribute']:\n",
    "            return dataset \n",
    "        else:\n",
    "            return dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    return _input_fn\n",
    "\n",
    "write_py(make_csv_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'AIRLINE': array([1, 1], dtype=int32),\n",
       "   'ARR': array([48, 48], dtype=int32),\n",
       "   'ARR_LAT': array([36.89, 36.89], dtype=float32),\n",
       "   'ARR_LON': array([-76.2, -76.2], dtype=float32),\n",
       "   'DEP_DELAY': array([0.09733124, 0.18210362], dtype=float32),\n",
       "   'DEP_DOW': array([6, 6], dtype=int32),\n",
       "   'DEP_HOD': array([15, 22], dtype=int32),\n",
       "   'DEP_LAT': array([33.63, 33.63], dtype=float32),\n",
       "   'DEP_LON': array([-84.42, -84.42], dtype=float32),\n",
       "   'DIFF_LAT': array([0.44145387, 0.44145387], dtype=float32),\n",
       "   'DIFF_LON': array([0.87757736, 0.87757736], dtype=float32),\n",
       "   'DISTANCE': array([0.09876987, 0.09876987], dtype=float32),\n",
       "   'MEAN_TEMP_ARR': array([0.5162454 , 0.30505416], dtype=float32),\n",
       "   'MEAN_TEMP_DEP': array([0.84873927, 0.394958  ], dtype=float32),\n",
       "   'MEAN_VIS_ARR': array([0.45698923, 0.4892473 ], dtype=float32),\n",
       "   'MEAN_VIS_DEP': array([1.        , 0.98437494], dtype=float32),\n",
       "   'WND_SPD_ARR': array([0.00540054, 0.00820082], dtype=float32),\n",
       "   'WND_SPD_DEP': array([0.37209302, 0.33333334], dtype=float32)},\n",
       "  array([22., 46.], dtype=float32))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_input_fn = make_input_fns()['csv']\n",
    "NUM_BATCHES=1\n",
    "with tf.Session() as sess:\n",
    "    train_input_fn = make_input_fn(\"./testdata/test.csv\", batch_size = 2, \n",
    "                                   options={'shuffle_buffer_size': 10, 'distribute':False})\n",
    "    input = train_input_fn()\n",
    "    res = [sess.run(input) for i in range(NUM_BATCHES)]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Read from TFRecords File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make_tfr_input_fn written to ./train/make_tfr_input_fn.py.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_tfr_input_fn(filename_pattern, batch_size, options):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from train.model_config import LABEL_COLUMN\n",
    "    from train.model_config import TRAINING_METADATA\n",
    "\n",
    "    feature_spec = TRAINING_METADATA.schema.as_feature_spec()\n",
    "\n",
    "    def _input_fn():\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern=filename_pattern,\n",
    "            batch_size=batch_size,\n",
    "            features=feature_spec,\n",
    "            shuffle_buffer_size=options['shuffle_buffer_size'],\n",
    "            prefetch_buffer_size=options['prefetch_buffer_size'],\n",
    "            reader_num_threads=options['reader_num_threads'],\n",
    "            parser_num_threads=options['parser_num_threads'],\n",
    "            sloppy_ordering=options['sloppy_ordering'],\n",
    "            label_key=LABEL_COLUMN)\n",
    "\n",
    "        if options['distribute']:\n",
    "            return dataset \n",
    "        else:\n",
    "            return dataset.make_one_shot_iterator().get_next()\n",
    "    return _input_fn\n",
    "\n",
    "write_py(make_tfr_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'AIRLINE': array([1, 1]),\n",
       "   'ARR': array([48, 87]),\n",
       "   'ARR_LAT': array([36.89, 41.3 ], dtype=float32),\n",
       "   'ARR_LON': array([-76.2 , -95.89], dtype=float32),\n",
       "   'DEP_DELAY': array([0.09733124, 0.09733124], dtype=float32),\n",
       "   'DEP_DOW': array([6, 1]),\n",
       "   'DEP_HOD': array([15, 16]),\n",
       "   'DEP_LAT': array([33.63, 33.63], dtype=float32),\n",
       "   'DEP_LON': array([-84.42, -84.42], dtype=float32),\n",
       "   'DIFF_LAT': array([0.44145387, 0.5429031 ], dtype=float32),\n",
       "   'DIFF_LON': array([0.87757736, 0.6661297 ], dtype=float32),\n",
       "   'DISTANCE': array([0.09876987, 0.16803624], dtype=float32),\n",
       "   'MEAN_TEMP_ARR': array([0.5162454 , 0.48916963], dtype=float32),\n",
       "   'MEAN_TEMP_DEP': array([0.84873927, 0.36974797], dtype=float32),\n",
       "   'MEAN_VIS_ARR': array([0.45698923, 0.4892473 ], dtype=float32),\n",
       "   'MEAN_VIS_DEP': array([1.        , 0.96875006], dtype=float32),\n",
       "   'WND_SPD_ARR': array([0.00540054, 0.00860086], dtype=float32),\n",
       "   'WND_SPD_DEP': array([0.37209302, 0.3875969 ], dtype=float32)},\n",
       "  array([22., 21.], dtype=float32))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_input_fn = make_input_fns()['tfr']\n",
    "with tf.Session() as sess:\n",
    "    train_input_fn = make_input_fn(\n",
    "        './testdata/test.tfr',\n",
    "        batch_size=2,                 \n",
    "        options={'shuffle_buffer_size': 10,\n",
    "                'prefetch_buffer_size': 10,\n",
    "                'reader_num_threads': 2,\n",
    "                'parser_num_threads': 2,\n",
    "                'sloppy_ordering': True,\n",
    "                'distribute': False})\n",
    "    input = train_input_fn()\n",
    "    res = [sess.run(input) for i in range(1)]\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
